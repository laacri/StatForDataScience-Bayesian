---
title: "Bayesian inference of CPU benchmarks"
subtitle: "SDS II Final Project"
author: "Cristiana Di Tullio"
date: "11 September 2024"
output: 
  html_document:
    theme: journal
    css: seagreen3_style.css
    toc: yes
    toc_float:
      collapsed: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r eval=FALSE, message=FALSE, include=FALSE}
# De-comment if necessary to run it
setwd("~\\Desktop\\Uni\\Data Science\\SDS II\\2024\\FINAL PROJECT")
```

```{r libraries, include=FALSE}
# Import libraries
library("png")       #
library(readr)
library(tidyverse)
library(ggplot2)
library(dplyr)
library(tidyr)
library(DT)
library(VIM)
library(viridis)
library(ggcorrplot)
library(GGally)
library(R2jags)
library(mcmcplots)   #
library(ggmcmc)
library(ggthemes)    #
library(plotly)      #
library(mcmc)        #
library(coda)        #
library(loo)         #
library(gridExtra)
library(knitr)
```

# 1. Introduction

This project's aim consists in carrying out a **regression task** on a dataset containing CPU benchmarks data.

A *Central Processing Unit (CPU)* is a complex set of electronic circuitry that represents the primary control center of a computer and what enables it to execute instructions. In an era of rapidly advancing technology and increasing needs for computational power, these small hardware components stand out as protagonists: they are what allows both users and manufacturers to implement and run simple operations as well as complex artificial intelligence models simply from our desks, in many cases without us even being aware of it. They contain all the intelligence, logic and power of computers. They are designed in order to realize many desirable properties both from a software and a hardware point of view and different models are optimized for different kinds of applications. <br> *Benchmark scores* allow for an easy comparison between different processors by grading their performance on a standardized series of tests, and they are useful in many instances: when buying or building a new PC, when choosing the hardware components to build a specialized hardware architecture.

For our purposes, we will use the Bayesian modeling machinery to estimate the relationship between the `price` target variable and the predictor variables contained in the dataset, fitting models of different complexity to see which is the best one and what variables appear to be more important in the prediction in relation to the chosen model.

```{r load_file, include=FALSE}
# Load dataset
cpu <- read_csv("./CPU_benchmark.csv")
cpu_df <- as.data.frame(cpu)
rows <- dim(cpu_df)[1]
cols <- dim(cpu_df)[2]
```


# 2. Dataset description and preprocessing

The original dataset comprises `r rows` observations of single socket CPU processors and `r cols` features containing their specifications and benchmark scores, that measure their performance on a standardized series of tests. <br>
Let's start exploring the variables:

| Variable      | Type      | Description                                                                               |
|------------|------------|------------------------------------------------|
| `cpuName`     | character | Full commercial name of the specific CPU model.                                           |
| `price`       | numeric   | Last seen market price of the CPU in United States dollars (\$).                          |
| `cpuMark`     | numeric   | Multi-thread CPU Mark obtained as average score from eight different tests on all cores.  |
| `cpuValue`    | numeric   | CPU value indicator, obtained as `cpuMark` / `price`.                                     |
| `threadMark`  | numeric   | Single-thread CPU Mark obtained as average score from eight different tests on all cores. |
| `threadValue` | numeric   | Thread value indicator, obtained as `threadMark` / `price`.                               |
| `TDP`         | numeric   | Thermal Design Power in Watt (W), maximum amount of heat generated by a CPU.              |
| `powerPerf`   | numeric   | Power performance of the CPU, obtained as `cpuMark` / `TDP`.                              |
| `cores`       | integer   | Number of cores the processor is equipped with.                                           |
| `testDate`    | integer   | Year corresponding to the date of first benchmark score submission.                       |
| `socket`      | character | Type of the CPU socket, its physical interface on the motherboard.                        |
| `category`    | character | Category of computers that the CPU is designed for.                                       |

We are dealing with a majority of numerical variables, and some categorical ones.

**BEWARE!** Some variables are obtained as a ratio of others, so they depend directly on them! <br>
Therefore, the relationships between these variables need to be interpreted *cum grano salis*. What's more, two of these variables are obtained directly from the target! This means that we won't be able to use them as predictors in the regression model, to ensure its integrity. Otherwise we would be using information coming from the target to predict the target itself, which would be methodologically wrong.

## 2.1 Missing Values

We need to check for missing values and eventually decide how to handle them.

```{r na_function, include=FALSE}
count_missing_values <- function(df) {
  na_counts <- sapply(df, function(x) sum(is.na(x)))
  na_df <- data.frame(Variable = names(na_counts), MissingValues = na_counts)
  kable(na_df, row.names = FALSE)
}
```

```{r}
count_missing_values(cpu_df)
```

There are indeed some missing values, and in strangely recurring amounts. Let's have a look:

```{r NA_plot, echo=FALSE}
missing_counts <- sapply(cpu_df, function(x) sum(is.na(x)))
missing_percentages <- (missing_counts / rows) * 100 

missing_data <- data.frame(
  variable = names(missing_counts),
  missing_count = missing_counts,
  missing_percentage = missing_percentages
)

ggplot(missing_data, aes(x = variable, y = missing_percentage)) +
  geom_bar(stat = "identity", fill = "#cd4390", alpha = 0.7) + # seagreen3
  labs(title = "Missing values per variable",
       x = "variable",
       y = "% missing") +
  ylim(0, 100) +
  theme(axis.text.x = element_text(angle = 45, hjust = 1),
        plot.title = element_text(hjust = 0.5))
```

Two groups of variables have exactly the same amount of missing values: the first is `cpuValue`, `threadValue` and `price`, and the second one is `powerPerf` and `TDP`. This is due to the fact that some variables are derived from others: any missing value in `price` or `TDP` will automatically result in a missing value in `cpuValue`, `threadValue` or `powerPerf`. <br>
There are two ways to handle missing values: either dropping the observations containing some of them, or trying to impute them in a reasonable way. In order not to waste data, but also not to compromise our inferential models, here we'll do both:

-   **Drop missing `prices`**: since imputing the target variable can introduce bias and distortion in the model, possibly altering the true underlying relationships, we prefer to *drop* all the observations whose price is not available. Even if this means discarding a considerable part of our data, we'll still have a good amount to carry out our analysis and of a better quality;

-   **Impute missing `TDP`**: on the other hand, the missing values in `TDP` are less problematic and can be replaced. We will set them equal to the mean `TDP` value of the $k = 5$ most similar observations in the dataset, where the similarity is computed with an Euclidean distance metric. Such imputation can be performed with the `KNN` function provided by the `VIM` R package.

After this preprocessing, we can check:

```{r missing_values, include=FALSE}
# Handling missing values and duplicates
# remove duplicate entries
cpu_df <- cpu_df %>% distinct()
# drop observations with missing price
cpu_df <- cpu_df %>% drop_na(price)
# impute missing values in TDP
cpu_df <- kNN(cpu_df, variable = "TDP", k = 5)
cpu_df <- cpu_df[, 1:length(cpu_df) - 1]
cpu_df$powerPerf <- ifelse(is.na(cpu_df$powerPerf), cpu_df$cpuMark / cpu_df$TDP, cpu_df$powerPerf)
# order the dataset by testDate
cpu_df <- cpu_df[order(cpu_df$testDate), ]
rownames(cpu_df) <- NULL
```

```{r}
count_missing_values(cpu_df)
```

## 2.2 Categorical variables

```{r as_factor_categorical, include=FALSE}
# pre-processing of categorical variables
cpu_df$cpuName  <- as.factor(cpu_df$cpuName)
cpu_df$socket   <- as.factor(cpu_df$socket)
cpu_df$category <- as.factor(cpu_df$category)
```

Let's start taking a closer look at our features. The categorical variables in our dataset are *nominal* and are:

-   `cpuName`: just represents the processor name and it's unique for all the observations in the dataset, so we can consider it not differently than just an index. Although, we can inspect it briefly to get a glimpse of the major manufacturer companies in our original available data:

```{r brands_plot, echo=FALSE}
brand_df <- cpu %>%
            mutate(manufacturer = case_when(
              grepl("Intel", cpu$cpuName, ignore.case = TRUE) ~ "Intel",
              grepl("AMD", cpu$cpuName, ignore.case = TRUE) ~ "AMD",
              grepl("MediaTek", cpu$cpuName, ignore.case = TRUE) ~ "MediaTek",
              grepl("Qualcomm", cpu$cpuName, ignore.case = TRUE) ~ "Qualcomm",
              grepl("ARM", cpu$cpuName, ignore.case = TRUE) ~ "ARM",
              grepl("Samsung", cpu$cpuName, ignore.case = TRUE) ~ "Samsung",
              TRUE ~ "Other"))

brand_counts <- brand_df %>%
                       group_by(manufacturer) %>%
                       summarise(count = n())

# Create the bar plot
ggplot(brand_counts, aes(x = manufacturer, y = count, fill = manufacturer)) +
  geom_bar(stat = "identity", alpha = 0.8) +
  labs(title = "Major manufacturers among original data",
       x = "manufacturer",
       y = "CPUs") +
  scale_fill_viridis_d() +
  theme(plot.title = element_text(hjust = 0.5))
```

So, our dataset contains mostly data about Intel and AMD processors (`r round((nrow(cpu_df[grepl("Intel", cpu_df$cpuName, ignore.case = TRUE), ]) + nrow(cpu_df[grepl("AMD", cpu_df$cpuName, ignore.case = TRUE), ])) / nrow(cpu_df), 3)*100`$\%$ of observations). It's not surprising, as these are the two leading companies in the market. We can't really include this variable in the model as it is, but from it we can derive a categorical feature `brand` containing the three levels Intel, AMD and Other (that also includes the minor companies). <br>
We then apply a one-hot encoding to this variable, obtaining the variables: `brandIntel`, `brandAMD` and `brandOther`.

```{r dummy_brand, include=FALSE}
# One-hot encoding of the variable "brand"
cpu_df$brandIntel <- ifelse(grepl("Intel", cpu_df$cpuName, ignore.case = TRUE), 1, 0)
cpu_df$brandAMD   <- ifelse(grepl("AMD", cpu_df$cpuName, ignore.case = TRUE), 1, 0)
cpu_df$brandOther <- ifelse(cpu_df$brandIntel == 0 & cpu_df$brandAMD == 0, 1, 0)
```


-   `socket`: let's see what are the most frequent levels and how many unique values are there.

```{r socket_plot, echo=FALSE}
# socket plot
cpu_df$socket <- droplevels(cpu_df$socket)

ggplot(cpu_df, aes(x = socket, fill = socket)) +
geom_bar(alpha = 0.7) +
scale_fill_viridis_d() +
labs(x = "socket", y = "count") +
ggtitle("Socket barplot") +
theme(plot.title = element_text(hjust = 0.5),
      axis.text.x = element_text(angle = 45, hjust = 1),
      legend.position = "none") 
```

The `socket` variable has way too many levels, as we can see in the barplot. It shows several peaks, so there aren't just few types of socket that stand out among the others. One-hot encoding this variable would imply making our data extremely sparse and increasing dramatically its dimensionality. For the sake of simplicity, this variable shall not be considered in the regression model that we'll implement.

- `category`: let's visualize the frequency of this variable's levels.

```{r category_plot, echo=FALSE}
# category plot
ggplot(cpu_df, aes(x = category, fill = category)) +
geom_bar(alpha = 0.7) +
scale_fill_viridis_d() +
labs(x = "category", y = "count") +
ggtitle("Category barplot") +
theme(plot.title = element_text(hjust = 0.5),
      axis.text.x = element_text(angle = 45, hjust = 1),
      legend.position = "none")
```

Now, this variable has `r nlevels(cpu_df$category)` a smaller number of levels and the majority of data (`r round((nrow(cpu_df[cpu_df$category == 'Desktop', ]) + nrow(cpu_df[cpu_df$category == 'Laptop', ]) + nrow(cpu_df[cpu_df$category == 'Server', ])) / nrow(cpu_df), 2) * 100`$\%$) is a `Desktop`, `Laptop` or `Server` type of CPU. Therefore, we proceed on one-hot encoding encode these three main levels in the new variables `categoryDesktop`, `categoryLaptop`, and `categoryServer` to be included in our model.

```{r filter_category, include=FALSE}
# One-hot encoding the variable "category"
cpu_df$categoryDesktop <- ifelse(cpu_df$category == "Desktop", 1, 0)
cpu_df$categoryLaptop  <- ifelse(cpu_df$category == "Laptop", 1, 0)
cpu_df$categoryServer  <- ifelse(cpu_df$category == "Server", 1, 0)
# all the rest is considered as reference level (it's 4 rows anyway)
```

## 2.3 Numerical variables

The subset of numerical variables contains our target and its predictors. In order to better understand them, we start inspecting their statistic summary:

```{r summary_num, echo=FALSE}
summary(cpu_df[, c("price", "cores", "cpuMark",
                   "cpuValue", "threadMark", "threadValue",
                   "TDP", "powerPerf", "testDate")])
```

We already have some insights about our data: for example, we can see that the average `price` of a CPU in this dataset is around $440\$$, from a minimum of $4\$$ to a maximum of about $9000\$$. The number of `cores` goes from a minimum of $1$ to a maximum of $80$, with the average CPU having $4$ cores. These CPUs have been tested starting from $2007$ up to $2022$, with the mean being $2014$, and so on. We can also notice that, in some cases, the maximum value is quite far from the median, probably indicating the presence of long-tail distributions. But let's plot the data, to have a clearer idea of the situation:


```{r num_vars_histogram, echo=FALSE}
# numerical variables histogram
# boolean mask of numeric variables
numeric_vars <- sapply(cpu_df, is.numeric)
# discard dummy variables (need their integer index)
numeric_vars <- numeric_vars[1:(length(numeric_vars) - 6)]
numeric_indices <- which(numeric_vars)
cpu_df_num_long <- tidyr::gather(cpu_df[, numeric_indices] , key = "variable", value = "value")

ggplot(cpu_df_num_long, aes(x = value, fill = variable)) +
  geom_histogram(binwidth = NULL, bins = 31, color = "black", alpha = 0.6) +
  facet_wrap(~ variable, scales = "free") +
  scale_fill_viridis_d() +
  labs(x = "variables", y = "frequency") +
  ggtitle("Numeric variables distributions") +
  theme(plot.title = element_text(hjust = 0.5),
        strip.text = element_text(size = 10, angle = 0, hjust = 0.5, vjust = 0.5),
        legend.position = "none")
```

As it appears, some numerical variables have really asymmetric distributions: specifically `cores`, `powerPerf`, `cpuMark`, `cpuValue`, `threadValue` and the target `price` itself are left-skewed with long right tails in some cases. <br>
Let's inspect the boxplots as well, that allow for a clearer view of the relative scale of variables and their outliers as a plus:

```{r num_var_boxplots, echo=FALSE}
palette_colors <- viridis::viridis_pal()(11)
palette_colors <- palette_colors[-1]
palette_colors <- palette_colors[-1]

ggplot(cpu_df_num_long, aes(x = variable, y = value, fill = variable)) +
  geom_boxplot(alpha = 0.6) +
  facet_wrap(~ variable, scales = "free", ncol = 3) +
  scale_fill_manual(values = palette_colors) +
  labs(x = "variables", y = "value") +
  ggtitle("Numeric variables boxplots") +
  theme(plot.title = element_text(hjust = 0.5),
        strip.text = element_text(size = 10, angle = 0, hjust = 0.5, vjust = 0.5),
        axis.text.x = element_blank(),
        axis.ticks.x = element_blank(),
        legend.position = "none")
```

Our suspects are confirmed: the variables have very different scales, and some of them many outliers. <br>
In these cases, it's a good practice to *log-transform* the variables. There are multiple benefits of this transformation: in addition to making the distributions more symmetric, it also reduces the impact of outliers by compressing the range of the data and linearizes relationships that would appear less intuitive in the original scale of the variables. We use the logarithm in base $10$ and, after the transformation, this is what the distributions of interest look like:

```{r log_transformation, echo=FALSE}
# Log10 transform the skewed variables
cpu_df$log10_cores       <- log10(cpu_df$cores + 1)
cpu_df$log10_price       <- log10(cpu_df$price + 1)
cpu_df$log10_threadValue <- log10(cpu_df$threadValue + 1)
cpu_df$log10_powerPerf   <- log10(cpu_df$powerPerf + 1)
cpu_df$log10_cpuMark     <- log10(cpu_df$cpuMark + 1)
cpu_df$log10_cpuValue    <- log10(cpu_df$cpuValue + 1)

cpu_df_log <- cpu_df %>%
              select(log10_cores, log10_price, log10_threadValue,
                     log10_powerPerf, log10_cpuMark, log10_cpuValue)
cpu_df_log_long <- tidyr::gather(cpu_df_log, key = "variable", value = "value")

# Histogram of log10-transformed variables
ggplot(cpu_df_log_long, aes(x = value, fill = variable)) +
  geom_histogram(binwidth = NULL, bins = 31, color = "black", alpha = 0.6) +
  facet_wrap(~ variable, scales = "free") +
  scale_fill_viridis_d() +
  labs(x = "variables", y = "frequency") +
  ggtitle("Log10 variables distributions") +
  theme(plot.title = element_text(hjust = 0.5),
        strip.text = element_text(size = 10, angle = 0, hjust = 0.5, vjust = 0.5),
        legend.position = "none")
```

```{r echo=FALSE}
# Boxplot of log10-transformed variables
ggplot(cpu_df_log_long, aes(x = variable, y = value, fill = variable)) +
  geom_boxplot(alpha = 0.6) +
  scale_fill_viridis_d() +
  labs(x = "variable", y = "value") +
  ggtitle("Log10 variables boxplot") +
  theme(plot.title = element_text(hjust = 0.5),
        axis.text.x = element_text(angle = 45, hjust = 1),
        axis.ticks.x = element_blank(),
        legend.position = "none")
```

## 2.4 Correlation

Before implementing the models, let's inspect the correlation matrix. <br>
This is a fundamental tool in evaluating the strength and direction of *linear* relationships between variables, understand better their interactions and detect possible multicollinearities. Since part of the variables have been transformed but the others still have a different scale, we proceed to standardize them before plotting the correlation matrix.

```{r log10_and_correlation, echo=FALSE, fig.width=10, fig.height=10}
# Log10 transform the remaining (non-skewed) variables
# cpu_df$log10_testDate    <- log10(cpu_df$testDate + 1)
# cpu_df$log10_threadMark  <- log10(cpu_df$threadMark + 1)
# cpu_df$log10_TDP         <- log10(cpu_df$TDP + 1)
# Standardizing the remaining (non-skewed) variables
cpu_df$scaled_testDate    <- as.numeric(scale(cpu_df$testDate))
cpu_df$scaled_threadMark  <- as.numeric(scale(cpu_df$threadMark))
cpu_df$scaled_TDP         <- as.numeric(scale(cpu_df$TDP))

standard_df <- cpu_df %>%
               select(log10_price, log10_cores, log10_threadValue,
                      log10_powerPerf, log10_cpuMark, log10_cpuValue, 
                      scaled_testDate, scaled_threadMark, scaled_TDP, #log10_testDate, log10_threadMark, log10_TDP
                      brandIntel, brandAMD, brandOther,
                      categoryDesktop, categoryLaptop, categoryServer) 

cor_matrix_standard <- cor(standard_df)

ggcorrplot(cor_matrix_standard, type = "lower", # hc.order = TRUE, 
           show.diag = TRUE, lab = TRUE, title = "Correlation matrix") +
  theme(plot.title = element_text(hjust = 0.5))
```

Remarks:

- **Positive correlations**: the target seems particularly correlated with `cores`, `powerPerf` and `cpuMark`;
- **Negative correlations**: the highest negative correlations are with `cpuValue` and especially `threadValue`, just as we expect: both of them contain the target in their denominator, so they will tend to decrease as it increases;
- **Potential multicollinearity**: a suspiciously high correlation value stands between `brandIntel` and `brandAMD`: they are almost perfectly negatively correlated. This could introduce multicollinearity in our model, causing the coefficients estimates to be unstable. Thus, just one of these two variables shall be included in the following section.

Before moving on to the Bayesian modeling, it's interesting to show the pair plots of the numerical variables to have a visual reference of how they interact with each other: this could be helpful to notice non-linear relationships and consider whether to include them in our models.

```{r pair_plot, echo=FALSE}
# Pair plots of numerical variables
#colnames(standard_df) <- gsub("log10_", "", colnames(standard_df))
colnames(standard_df) <- gsub("log10_|scaled_", "", colnames(standard_df))

ggpairs(standard_df[, c("price","cores", "powerPerf", "cpuMark", "testDate", "threadMark", "TDP")],
        title = "Pair plots of numerical variables") +
  theme(plot.title = element_text(hjust = 0.5))
```

There aren't clear trends of non-linearity: however, the interactions between some predictors like `cores` versus `cpuMark`, `TDP` versus `cpuMark` and `TDP` versus `cores` will be further inspected later.

```{r scaling, include=FALSE}
# cpu_df$scaled_testDate    <- as.numeric(scale(cpu_df$testDate))
# cpu_df$scaled_threadMark  <- as.numeric(scale(cpu_df$threadMark))
# cpu_df$scaled_TDP         <- as.numeric(scale(cpu_df$TDP))

# standard_df <- cpu_df %>%
#                select(log10_price, log10_cores, log10_threadValue,
#                       log10_powerPerf, log10_cpuMark, log10_cpuValue, 
#                       scaled_testDate, scaled_threadMark, scaled_TDP,
#                       brandIntel, brandAMD, brandOther,
#                       categoryDesktop, categoryLaptop, categoryServer)

#colnames(standard_df) <- gsub("log10_|scaled_", "", colnames(standard_df))
```

# 3. Bayesian modeling

The **Bayesian approach** differs from the frequentist in one fundamental assumption: there exist *prior* probability distributions associated to the unknown parameters to estimate, representing our initial knowledge or beliefs about them, that we are going to update conditionally as new data become available. <br>
The whole framework is grounded on Bayes' theorem:

$$
\pi(\theta | y) = \frac{L(y|\theta) \cdot \pi(\theta)}{p(y)} \propto L(y|\theta) \cdot \pi(\theta)
$$

where

-   $\pi(\theta | y)$ is the posterior distribution of the unknown parameter $\theta$;
-   $L(y|\theta)$ is the likelihood of the data given the unknown parameter;
-   $\pi(\theta)$ is the prior distribution of the unknown parameter.

Unless one is an expert in the domain to which the data belong or, somehow, is given a priori knowledge about the parameters behavior, the most common choice is to go for **"vague" uninformative priors**. These distributions are characterized by high variance, which lets them free to assume a wide range of values and lets the posterior distribution be modeled rather by the observed data than the prior itself.

In the **Normal Regression model**, the likelihood is shaped as a Gaussian distribution centered at a linear combination of predictors and regression coefficients. In matrix notation:

$$
y \,|\, X, \beta, \sigma^2 \; \sim \; MVN(\beta^TX, \; \sigma^2I)
$$
where

-   $y \,|\, X, \beta, \sigma^2$ is a Multivariate Normal Distribution;
-   $\beta$ is the matrix of regression coefficients;
-   $X$ is the feature matrix;
-   $\sigma$ is the standard deviation of.

We're getting to the heart of our analysis. We'll define three different models, fit them to the data and let the Monte Carlo Markov Chains (MCMC) sampling estimate suitable regression coefficients. To implement this, we used the `JAGS` software wrapped into the `R2jags` R package with the following parameters fixed for all models:

- $3$ MCMC chains;
- $10000$ iterations;
- $2000$ burn-in;
- $1$ thinning interval;

```{r}
n_chains <- 3
n_iter   <- 10000
burn_in  <- 2000
thin     <- 1
```

## 3.1 Model 1: purely linear

For the first model, we want to stick to linearity: all the predictors are included via a simple linear combination defined by a parameter $beta_i$. As mentioned before, the variables `cpuValue` and `threadValue` are excluded as they incorporate information directly from the target `price`. Also the variable `powerPerf` is left out, as it represents an interaction term between `cpuMark` and `TDP`. Moreover, we include just `brandIntel` among the brand hot encoding values, to avoid multicollinearity and redundancy. <br>
The resulting likelihood is written as:

$$
y \;|\;\beta, X = \beta_0 + \beta_1 \cdot \text{cores} + \beta_2 \cdot \text{cpuMark} + \beta_3 \cdot \text{testDate} + \\ + \beta_4 \cdot \text{threadMark} + \beta_5 \cdot \text{TDP} + \beta_6 \cdot \text{brandIntel} +  \\ + \beta_7 \cdot\text{catDesktop} + \beta_8 \cdot \text{catLaptop} + \beta_9 \cdot \text{catServer}
$$

In JAGS syntax, this gets translated as:

```{r model1.txt, echo=FALSE}
file_content <- readLines("model1.txt")
cat(file_content, sep = "\n")
```

```{r jags1_run, eval=TRUE, results='hide'}
model_data_1 <- list(price = standard_df$price,                     # Y
                     cores = standard_df$cores,                     # x1
                     cpuMark = standard_df$cpuMark,                 # x2
                     testDate = standard_df$testDate,               # x3
                     threadMark = standard_df$threadMark,           # x4
                     TDP = standard_df$TDP,                         # x5
                     brandIntel = standard_df$brandIntel,           # x6
                     catDesktop = standard_df$categoryDesktop,      # x7
                     catLaptop = standard_df$categoryLaptop,        # x8
                     catServer = standard_df$categoryServer,        # x9
                     N = nrow(standard_df))                         # N

params1 <- c("beta0", "beta1", "beta2", "beta3", "beta4", "beta5",
             "beta6", "beta7", "beta8", "beta9", "sigma")

set.seed(73)
cpujags1 <- jags(data = model_data_1,
              parameters.to.save = params1,
              model.file = "model1.txt",
              n.chains = n_chains,
              n.iter = n_iter,
              n.burnin = burn_in,
              n.thin = thin)
```

```{r}
cpujags1
```

The JAGS result provides an overview of point estimates, $95\%$ credible intervals (from the $2.5\%$ to the $97.5\%$ quantile) and convergence metrics for our coefficient estimates. At a first look, it seems that the convergence went well. Most parameters have been estimated with a meaningful value, but some of them (namely $\beta_4$ and $\beta_9$, coefficients of $\text{threadMark}$ and $\text{catServer}$) have a credible interval that contains zero. This undermines the significance of the coefficients, because zero appears to be a plausible value for them. <br>
Thus, in the next model we tried to exclude the correspondent variables to see if results would improve.


## 3.2 Model 2: introducing nonlinearity

For the second model, we build up on the first one discarding the variables `threadMark` and `catServer` (not significant) and adding back the variable `powerPerf`. This last predictor introduces nonlinearity, being an interaction term between `cpuMark` and `TDP`. We want to see how much results are different just by doing this small changes. Now, the likelihood is written as:

$$
y \;|\;\beta, X = \beta_0 + \beta_1 \cdot \text{cores} + \beta_2 \cdot \text{cpuMark} + \beta_3 \cdot \text{testDate} + \\ + \beta_5 \cdot \text{TDP} + \beta_6 \cdot \text{brandIntel} + \beta_7 \cdot\text{catDesktop} + \beta_8 \cdot \text{catLaptop} +  \\
+ \beta_{10} \cdot \text{powerPerf}
$$

And the JAGS specification:

```{r model2.txt, echo=FALSE}
file_content <- readLines("model2.txt")
cat(file_content, sep = "\n")
```

```{r jags2_run, eval=TRUE, results='hide'}
model_data_2 <- list(price = standard_df$price,                     # Y
                     cores = standard_df$cores,                     # x1
                     cpuMark = standard_df$cpuMark,                 # x2
                     testDate = standard_df$testDate,               # x3
                     TDP = standard_df$TDP,                         # x5
                     brandIntel = standard_df$brandIntel,           # x6
                     catDesktop = standard_df$categoryDesktop,      # x7
                     catLaptop = standard_df$categoryLaptop,        # x8
                     powerPerf = standard_df$powerPerf,             # x10
                     N = nrow(standard_df))                         # N

# Parameters to monitor (the regression coefficients)
params2 <- c("beta0", "beta1", "beta2", "beta3", "beta5",
             "beta6", "beta7", "beta8", "beta10", "sigma")

set.seed(73)
cpujags2 <- jags(data = model_data_2,
              parameters.to.save = params2,
              model.file = "model2test.txt",
              n.chains = n_chains,
              n.iter = n_iter,
              n.burnin = burn_in,
              n.thin = thin)
```

```{r}
cpujags2
```

Again it appears that there was no issue with the convergence. <br>
We can see that the DIC dropped by about `r round(cpujags1$BUGSoutput$DIC - cpujags2$BUGSoutput$DIC)` units, which looks like an improvement. No parameters have credible intervals that contain $0$, so we can consider all of them as significant. In the next model, we explore more nonlinearity doing different tries with interaction terms.

## 3.3 Model 3: even more nonlinearity

For the last model, we tried to increase nonlinearity to see how the model would react and if there would be any improvement in the coefficient estimates and model deviance. Some interactions that looked somewhat promising in the pair plots have been further investigated to see if it could make sense to include them in the model:

```{r echo=FALSE}
# possible interaction 1
# price VS cores*cpuMark
ggplot(standard_df, aes(x = cores * cpuMark, y = price)) +
  geom_point(aes(color = cores * cpuMark), alpha = 0.7) +
  scale_color_viridis_c(guide = "none") +
  labs(x = "cores * cpuMark", y = "price") +
  ggtitle("price VS cores*cpuMark") +
  theme(plot.title = element_text(hjust = 0.5))

# possible interaction 2
# price VS TDP*cpuMark
ggplot(standard_df, aes(x = TDP * cpuMark, y = price)) +
  geom_point(aes(color = TDP * cpuMark), alpha = 0.7) +
  scale_color_viridis_c(guide = "none") +
  labs(x = "TDP * cpuMark", y = "price") +
  ggtitle("price VS TDP*cpuMark") +
  theme(plot.title = element_text(hjust = 0.5))

# possible interaction 3
# price VS TDP*cores
ggplot(standard_df, aes(x = TDP * cores, y = price)) +
  geom_point(aes(color = TDP * cores), alpha = 0.7) +
  scale_color_viridis_c(guide = "none") +
  labs(x = "TDP * cores", y = "price") +
  ggtitle("price VS TDP*cores") +
  theme(plot.title = element_text(hjust = 0.5))

#grid.arrange(plot1, plot2, plot3, nrow = 3)
```

It doesn't really look like there is a specific, even nonlinear, function between the target variable and these interaction terms. However, after a long trial-and-error phase that tested many model variations to understand which parameter combination and what functions could work best for our purposes, some of these interactions have proved interesting. The final model is reported below.

$$
y \;|\;\beta, X \;=\; \beta_0 + \beta_1 \cdot \text{cores} + \beta_{12} \cdot [\;\log(\text{cores}\cdot\text{cpuMark})\;] + \beta_2 \cdot \text{cpuMark} + \\
+\, \beta_{2sq} \cdot \text{cpuMark}^2 + \beta_5 \cdot \text{TDP} + \beta_{5sq} \cdot \text{TDP}^2 + \beta_{52} \cdot (\text{TDP} \cdot \text{cpuMark}) + \\
+\, \beta_6 \cdot \text{brandIntel} + \beta_7 \cdot \text{catDesktop} + \beta_8 \cdot \text{catLaptop} + \\
+\, \beta_{10} \cdot \text{powerPerf} + \beta_{10sq} \cdot \text{powerPerf}^2
$$

The JAGS specification:

```{r model3.txt, echo=FALSE}
file_content <- readLines("model3.txt")
cat(file_content, sep = "\n")
```

```{r jags3_run, eval=TRUE, results='hide'}
model_data_3 <- list(price = standard_df$price,                     # Y
                     cores = standard_df$cores,                     # x1
                     cpuMark = standard_df$cpuMark,                 # x2
                     TDP = standard_df$TDP,                         # x5
                     brandIntel = standard_df$brandIntel,           # x6
                     catDesktop = standard_df$categoryDesktop,      # x7
                     catLaptop = standard_df$categoryLaptop,        # x8
                     powerPerf = standard_df$powerPerf,             # x10
                     N = nrow(standard_df))                         # N
  

params3 <- c("beta0", "beta1", "beta12", "beta2", "beta2_sq", 
             "beta5", "beta5_sq", "beta52", "beta6",
             "beta7", "beta8", "beta10", "beta10_sq", "sigma")

set.seed(73)
cpujags3 <- jags(data = model_data_3,
              parameters.to.save = params3,
              model.file = "model3test.txt",
              n.chains = n_chains,
              n.iter = n_iter,
              n.burnin = burn_in,
              n.thin = thin)
```

```{r}
cpujags3
```

At this point, the DIC has been reduced of another `r round(cpujags2$BUGSoutput$DIC - cpujags3$BUGSoutput$DIC)` units. The parameters have all valid point estimates and credible intervals. Some of the coefficients have really small estimates, but trying to remove them and running again the model has proved to just worsen the model performances.

# 4. Comparative frequentist analysis

The **Frequentist approach** to statistics interprets probability in terms of mere frequency of events occurring in repeated trials. In this context, inference of unknown parameters can be carried out observing the given data under different hypotheses, but it does not involve the concept of prior beliefs to update. <br>
Now we are curious to run a model using the classical regression technique and compare results. Since it is a fully linear model, we are going to compare it to the first Bayesian model run we implemented. 

```{r}
# Frequentist linear regression
model_freq <- lm(formula = price ~ cores +
                                   cpuMark +
                                   testDate +
                                   threadMark +
                                   TDP +
                                   brandIntel +
                                   categoryDesktop +
                                   categoryLaptop +
                                   categoryServer,
                 data = standard_df)
# Get a summary of results
summary(model_freq)
```

Comparison of frequentist and Bayesian estimates for linear regression:

| Coefficient     | Frequentist estimate | Std     | Bayesian Estimate | Std        |
|--------------|----------------|--------------|--------------|--------------|
| intercept       | 0.93758      | 0.20673    | 0.9313471       | 0.206527    |
| cores           | 0.62801      | 0.09242    | 0.6255931       | 0.09230711  |
| cpuMark         | 0.21458      | 0.07133    | 0.2166692       | 0.07139804  |
| testDate        | 0.13039      | 0.01601    | 0.1302564       | 0.0160569   |
| threadMark      | -0.02167     | 0.02510    | -0.02232104     | 0.02507232  |
| TDP             | 0.10468      | 0.01474    | 0.1048082       | 0.01477375  |
| brandIntel      | 0.13150      | 0.02180    | 0.1315548       | 0.02189265  |
| categoryDesktop | -0.10516     | 0.04110    | -0.1048176      | 0.04136357  |
| categoryLaptop  | 0.15225      | 0.04342    | 0.1530201       | 0.04379014  |
| categoryServer  | -0.01339     | 0.04486    | -0.01312861     | 0.04523161  |


We can see that the Frequentist approach reconstructed almost the exact same coefficient estimates returned by the Bayesian linear modeling, with similar values even for the standard error (equal until the second decimal digit). What's more, we can recognize more or less the same significance levels of our first model, with `threadMark` and `categoryServer` being the least significant variables of the linear model. So, we can conclude that the two approaches are consistent with each other, indicating that we carried out an overall sound analysis.

# 5. Model selection

## 5.1 A, B, D information criteria

These three criteria are all based on the fundamental concept of *deviance*: this is a goodness-of-fit measure for a statistical model and is defined as:

$$
D(\theta) \;=\; -2 \,\log (f(y \,|\, \theta)) \;=\; -2\,\log L(\theta)
$$

This is a well known concept even in the frequentist setting, where it identifies the deviance of the fitted model with respect to a perfect model with zero deviance. In the Bayesian context it can be thought of as a measure of how well the model, with given parameters $\theta$, explains the observed data $y$. Lower values of deviance indicate a better fit to the data. <br>

**Akaike Information Criterion (AIC)**: it's the most popular numeric index used to address model selection in the *frequentist* inference setting, and allows to choose models that have good out-of-sample predictive capabilities. Despite not being specific for the Bayesian setting, it can also be applied in this context to provide a baseline metric. AIC is defined as:

$$
AIC \;=\; D_{\hat{\theta}}(y) + 2 \cdot p
$$

where

-    $D_{\hat{\theta}}(y)$ is the *deviance* of the model evaluated at a ”representative” point $\hat{\theta}$ (usually the posterior mean);
-   $p$ is the *number of parameters* of the model.

**Bayes Information Criterion (BIC)**: is another criterion for model selection closely related to AIC, but compared to the latter it uses a larger penalty term for the number of parameters in the model and the increasing sample size. It is defined as follows:

$$
BIC \;=\; D_{\hat{\theta}}(y) + \ln(n) \cdot p\\
$$

where

-    $D_{\hat{\theta}}(y)$ is the *deviance* of the model evaluated at a ”representative” point $\hat{\theta}$ (usually the posterior mean);
-   $n$ is the number of samples;
-   $p$ is the *number of parameters* of the model.

**Deviance Information Criterion (DIC)**: is the main Bayesian measure for model selection, sort of an equivalent of AIC in this context. Like AIC, it trades off a measure of model adequacy against a measure of complexity, but unlike AIC, it takes prior information into account. It is defined as:

$$
DIC \;=\; D(\bar{\theta}) + 2 \cdot p_D
$$

where

-   $D(\bar{\theta})$ is the *deviance* of the model calculated at $\bar{\theta}$, the expectation of $\theta$;
-   $p_D = \overline{D(\theta)} - D(\bar{\theta})$ is the *effective number of parameters*, that measures the difference between the mean deviance and the deviance evaluated in the point of maximum posterior probability.

It's important to recall that DIC can be used only as a *comparative* index: there is no way of considering DIC on a standard, absolute scale. Therefore, it only makes sense when used to compare different models applied to the same dataset.

Let's compute and visualize these indexes values for all our models:

```{r model_selection, echo=FALSE, warning=FALSE}
# DIC is provided already by jags output
# AIC and BIC need to be computed
compute_AIC <- function(jags) {
  deviance <- jags$BUGSoutput$DIC - jags$BUGSoutput$pD
  aic <- deviance + 2 * jags$BUGSoutput$pD
  return(aic)
}
compute_BIC <- function(jags) {
  n <- nrow(cpu_df)
  deviance <- jags$BUGSoutput$DIC - jags$BUGSoutput$pD
  bic <- deviance + log(n) * jags$BUGSoutput$pD
  return(bic)
}
aic_1 <- compute_AIC(cpujags1)
aic_2 <- compute_AIC(cpujags2)
aic_3 <- compute_AIC(cpujags3)
bic_1 <- compute_BIC(cpujags1)
bic_2 <- compute_BIC(cpujags2)
bic_3 <- compute_BIC(cpujags3)
aic_freq <- AIC(model_freq)
bic_freq <- BIC(model_freq)

dic_values <- c(cpujags1$BUGSoutput$DIC, cpujags2$BUGSoutput$DIC, cpujags3$BUGSoutput$DIC) # no DIC for frequentist
aic_values <- c(aic_1, aic_2, aic_3)
bic_values <- c(bic_1, bic_2, bic_3)

model_names <- c("Model 1\n(linear)", "Model 2\n(interaction)", "Model 3\n(nonlinear)")
criteria <- c("DIC", "AIC", "BIC")

values <- data.frame(Model = rep(model_names, each = length(criteria)),
                     Criteria = rep(criteria, times = length(model_names)),
                     Value = c(dic_values[1], aic_values[1], bic_values[1],
                               dic_values[2], aic_values[2], bic_values[2],
                               dic_values[3], aic_values[3], bic_values[3]))
values <- rbind(values, data.frame(Model = rep("Model\n(frequentist)", each = length(criteria)),
                                   Criteria = criteria,
                                   Value = c(NA, aic_freq, bic_freq)))

custom_colors <- c("AIC" = "#cd4390", "BIC" = "#6495ed", "DIC" = "#43cd75")

ggplot(values, aes(x = Model, y = Value, fill = Criteria)) +
  geom_bar(stat = "identity", position = position_dodge()) +
  labs(title = "Model selection",
       x = "",
       y = "",
       fill = "Index") +
  scale_fill_manual(values = custom_colors) +
  theme(plot.title = element_text(hjust = 0.5),
        legend.position = "right")
```

The frequentist model doesn't have a DIC value, as it's a specific Bayesian measure. The best performing model under all the criteria is the third one we implemented. In the following sections, we'll focus just on this model and examine its convergence diagnostics.

# 6. Best model diagnostics

In this section we plot and comment the main aspects of the third model behavior during the MCMC sampling of coefficient estimates. First of all, let's have a look at the estimated coefficients. We can plot them together with their $68\%$ and $95\%$ credible intervals in the same figure to have a better understanding of their relative magnitude. The different colors indicate the different chains.

```{r caterplot, echo=FALSE}
caterplot(cpujags3, ask = FALSE,
          parms = c("beta0", "beta1", "beta12", "beta2", "beta2_sq",
                    "beta5", "beta5_sq", "beta52", "beta6",
                    "beta7", "beta8", "beta10", "beta10_sq", "sigma"),
          #c("beta0", "beta1", "beta12", "beta5", "beta52", "beta6"),
          collapse = FALSE) # try collapse = TRUE to collapse all chains into one
```

## 6.1 Traceplots

The traceplot shows the series of parameter values generated using MCMC sampling in the `r n_chains` different chains, each one corresponding to a different color. The series of values for each parameter is shown in a different panel. The three chains, represented by the orange, green and blue line plots respectively, largely overlap and this is reassuring as it means that they converge in distribution to the same values. <br>
Note that just `r n_iter - burn_in` iterations are included. The initial parameter values are discarded in the burn-in phase as they're the ones where the chains are sampling different regions of the parameter space, trying to reach to the stationary distribution. <br>

```{r traceplots, echo=FALSE}
mcmc_samples <- as.mcmc(cpujags3$BUGSoutput)
if (!inherits(mcmc_samples, "mcmc.list")) {
  mcmc_samples <- mcmc.list(mcmc_samples)
}
S <- ggs(mcmc_samples)
left_out_params <- c("variance", "deviance") # "sigma",
S_filtered <- S[!S$Parameter %in% left_out_params, ]

# ggs_traceplot(S_filtered) +
#   facet_wrap(~ Parameter, scales = "free", ncol = 1) +
#   labs(title = "Traceplots") +
#   theme(plot.title = element_text(hjust = 0.5))

# plot in batches to help readability
params_list <- split(unique(S_filtered$Parameter), ceiling(seq_along(unique(S_filtered$Parameter)) / 8))

for (i in seq_along(params_list)) {
  S_batch <- S_filtered[S_filtered$Parameter %in% params_list[[i]], ]
  
  traceplot <- ggs_traceplot(S_batch) +
    facet_wrap(~ Parameter, scales = "free", ncol = 2) # +
    #labs(title = paste("Traceplots - Batch", i)) +
    #theme(plot.title = element_text(hjust = 0.5))
  
  print(traceplot)
}
```

These traceplots suggest that the sampling procedure is doing a great job, quickly exploring the full range of the posterior distribution, so we can say that the chains are “well mixed”.

## 6.2 Density plots

Complementary to the traceplot, is the density plot, which provides information related to the posterior distribution of the various coefficients. In this case, convergence to a normal distribution for all three chains is immediate to notice.

```{r density_plots, echo=FALSE}
# ggs_density(S_filtered) +
#   facet_wrap(~ Parameter, scales = "free", ncol = 2) +
#   labs(title = "Density plots") +
#   theme(plot.title = element_text(hjust = 0.5))
# 

params_list <- split(unique(S_filtered$Parameter), 
                     ceiling(seq_along(unique(S_filtered$Parameter)) / 8))

# Plot each batch
for (i in seq_along(params_list)) {
  S_batch <- S_filtered[S_filtered$Parameter %in% params_list[[i]], ]
  
  density_plot <- ggs_density(S_batch) +
    facet_wrap(~ Parameter, scales = "free", ncol = 2) #+
    #labs(title = paste("Density Plots - Batch", i)) +
    #theme(plot.title = element_text(hjust = 0.5))
  
  print(density_plot)
}
```

## 6.3 Running Means

```{r running_means, echo=FALSE}
# ggs_running(S_filtered) +
#   facet_wrap(~ Parameter, scales = "free", ncol = 2) +
#   labs(title = "Running means") +
#   theme(plot.title = element_text(hjust = 0.5))

params_list <- split(unique(S_filtered$Parameter), 
                     ceiling(seq_along(unique(S_filtered$Parameter)) / 8))

# Plot each batch
for (i in seq_along(params_list)) {
  S_batch <- S_filtered[S_filtered$Parameter %in% params_list[[i]], ]
  
  running_means_plot <- ggs_running(S_batch) +
    facet_wrap(~ Parameter, scales = "free", ncol = 2) #+
    #labs(title = paste("Running Means - Batch", i)) +
    #theme(plot.title = element_text(hjust = 0.5))
  
  # Display the plot for the current batch
  print(running_means_plot)
}
```

We see that all the coefficients distributions converge quite quickly to their final posterior mean.

## 6.4 Autocorrelation

Autocorrelation measures the correlation between coefficient samples in subsequent iterations. It's a useful way to detect temporal dependence between sampled data points and it's directly connected to the *effective sample size* `n.eff` returned by the JAGS output. This is an estimate of the effectively independent samples obtained during the MCMC procedure. Higher values mean that we collected a lot of informative samples, whereas low values can be a symptom of some convergence issue. <br>
The following plots show autocorrelation for each coefficient.

```{r acf_plots, echo=FALSE}
# ggs_autocorrelation(S_filtered) +
#   facet_wrap(~ Parameter, scales = "free", ncol = 2) +
#   labs(title = "Autocorrelation") +
#   theme(plot.title = element_text(hjust = 0.5))

params_list <- split(unique(S_filtered$Parameter), 
                     ceiling(seq_along(unique(S_filtered$Parameter)) / 8))

# Plot each batch
for (i in seq_along(params_list)) {
  S_batch <- S_filtered[S_filtered$Parameter %in% params_list[[i]], ]
  
  autocorrelation_plot <- ggs_autocorrelation(S_batch) +
    facet_wrap(~ Parameter, scales = "free", ncol = 2) #+
    #labs(title = paste("Autocorrelation - Batch", i)) +
    #theme(plot.title = element_text(hjust = 0.5))
  
  # Display the plot for the current batch
  print(autocorrelation_plot)
}
```


## 6.5 R hat

Also known as the Gelman-Rubin statistics, it is another measure to assess the convergence of MCMC simulations and also part of the JAGS output. It indicates whether the different chains in the model have converged to a stable solution, ensuring the reliability and accuracy of your results. Its ideal value is $1$, which means indicating that all chains are sampling from the same underlying distribution. Let's see what ours looks like:

```{r rhat_plots, echo=FALSE}
ggs_Rhat(S_filtered)  +
  labs(title = "R hat") +
  theme(plot.title = element_text(hjust = 0.5))
```

In our case the R hat is $1$ for all parameters, so we can conclude that the convergence was successful and the simulation returned reliable results.

# 7. Conclusions

In this project, we carried out a regression task using Bayesian methods on real CPU Benchmark data. <br>
Various models were implemented to explore the relationship between CPU features and price, identifying more or less significant predictors and modeling complex interactions. Multiple metrics were used to assess model convergence and compare their performance. A frequentist model was also used to validate our findings, showing consistent results. <br>
Through this analysis, we not only gain insights into the factors mostly influencing CPU prices, but also demonstrate the power of the Bayesian machinery in tackling real-world data analysis problems.
